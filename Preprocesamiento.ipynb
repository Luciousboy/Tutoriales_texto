{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la utilización de diversos modelos debemos hacer un preprocesamiento de los textos que queremos analizar. Vamos a utilizar **Gensim** para hacer esta fase. \n",
    "\n",
    "Los pasos que vamos a seguir son los siguientes:\n",
    "\n",
    "- Cargar un conjunto de documentos.\n",
    "- Normalizar los documentos, quitando las palabras comunes y los caracteres extraños.\n",
    "- Construir un diccionario.\n",
    "\n",
    "### Construcción de un diccionario\n",
    "\n",
    "Comenzamos construyendo una lista con palabras comunes que no vamos a querer tener en cuenta en el análisis. Utlizamos la librería *codecs* para no tener problemas respecto de la codificación de los documentos, además de agregar como *header* el *encoding* explícitamente para no tener problemas al momento de escribir comentarios en el script o imprimir en pantalla palabras con tildes o caracteres raros. Las palabras comunes están especificadas en el archivo *stopwords_spanish* (separadas por el caracter *\\r\\n*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding: utf8 -*-\n",
    "\n",
    "# Carga de palabras comunes mediante la librería codecs\n",
    "import codecs \n",
    "stopwords_file = 'stopwords_spanish.txt'\n",
    "stopwords = codecs.open(stopwords_file,'r','utf8').read().split('\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionamos la lista *stopwords* para ver algunos de los términos cargados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'actualmente', u'ac\\xe1', u'ademas', u'adem\\xe1s', u'adrede', u'afirm\\xf3', u'agreg\\xf3', u'ahi', u'ahora']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La *u* delante de cada *string* especifica el [*encoding Unicode*](https://es.wikipedia.org/wiki/Codificaci%C3%B3n_de_caracteres) que maneja sin problemas todo tipo de caracteres (muchas veces cuando se escribe en python una frase con tildes hay que agregar la *u* delante, por ejemplo, u'además'). Al imprimir la lista entera, los términos se ven efectivamente codificados como *Unicode*, pero al imprimir en pantalla término a término vemos que todo funciona correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acá\n"
     ]
    }
   ],
   "source": [
    "print(stopwords[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos ahora los textos que vamos a utlizar tanto para la construcción del diccionario como para el análisis. Muchas veces va a ser de preferencia construir un diccionario (o mejor un modelo **Tf-idf** que veremos más adelante) con un corpus más grande que el conjunto de textos que vamos a analizar, así que no necesariamente debe coincidir el conjunto de textos utilizados para el preprocesamiento que para el análisis.\n",
    "\n",
    "Por el momento definimos la lista de documentos mediante una una [lista por compresión](http://elclubdelautodidacta.es/wp/2013/04/python-listas-por-comprension-1/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número total de documentos\n",
    "ndocs = 400\n",
    "\n",
    "# Carga de documentos\n",
    "documents = [codecs.open('Data/file{}.txt'.format(i),'r','utf8').read() for i in range(ndocs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el primer documento llamando al primer elemento de la lista definida anteriormente. Recordar que en python el primer elemento se denomina con el índice 0. Para no observar todo el documento entero vemos hasta el caracter 1000 incluido (que tiene el índice 999 ya que se cuenta desde 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varios famosos respaldaron a Muriel Santa Ana luego de haber contado que se realizó un aborto\n",
      "Facundo Arana intentó parecer un caballero, pero terminó desencadenando un alud de críticas. El actor aseguró, en medio de una entrevista, que  a su ex, Isabel Macedo, la veía doblemente realizada como mujer, primero por haber encontrado al hombre ideal, y después por estar a punto de dar a luz a su primer hijo . Las palabras del actor no pasaron desapercibidas para muchas de sus colegas, que salieron a retrucar sus dichos en Twitter. Una de las más enérgicas fue          Muriel Santa Ana  , quien contó que no está en sus planes casarse ni tener hijos, además de confesar que a los 24 años se realizó un aborto. Esta revelación generó una nueva polémica en las redes y en los medios, evidenciando una vez más que el derecho al aborto sigue siendo un tema que divide las aguas y que merece ser discutido y tratado en profundidad. Una de las primeras en brindar su apoyo a Santa Ana fue su amiga,      \n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es separar cada documento por palabras (es decir, descripto como 1-gramas. Bien podría extenderse a n-gramas). **Gensim** tiene la función *tokenize* que además tiene incorporadas otras funcionalidades como minimizar todas las letras mediante *lowercase = TRUE*, y elimina todos los términos que no están compuestos exclusivamente por caracteres alfabéticos, **incluyendo los números**. En caso de querer conservar los números se recomienda utilizar la función *word_tokenize* de la librería de análisis de lenguaje natural **Nltk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "# Separación en palabras y minimización de las palabras\n",
    "documents = [list(tokenize(doc, lowercase = True)) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, filtramos las palabras comunes presentes en la lista *stopwords* mediante la función *filter*. El criterio de la función *filter* como muchas de otras funciones de python mediante definiendo [funciones *lambda*](https://recursospython.com/guias-y-manuales/funciones-lambda/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización y limpieza de documentos: se descartan las palabras comunes.\n",
    "documents = [filter(lambda x: x not in stopwords, doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo! Tenemos los documentos dentro de un solo objeto, cada uno de ellos definidos a su vez como una lista cuyos elementos son todas la palabras minimizadas compuestas exclusivamente por caracteres alfabéticos y que no estaban incluídas dentro de la lista *stopwords*. \n",
    "Podemos ver cómo quedó descrito el primer documento llamando nuevamente al primer elemento de la lista *documents*. En este caso inspeccionamos los primeros 10 elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'famosos', u'respaldaron', u'muriel', u'santa', u'ana', u'contado', u'aborto', u'facundo', u'arana', u'intent\\xf3']\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso final es construir con todos estos términos un diccionario. Con **Gensim** esto se puede hacer fácilmente mediante la clase *Dictionary*, además de poder guardarlo para utilizarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Construccion del diccionario con gensim\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Guardado del diccionario\n",
    "dictionary.save('tutorial.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspección del diccionario\n",
    "\n",
    "El diccionario es un objeto el cual tenemos que tener siempre presente para consultar la relación entre índices y términos, y muchas veces es requerido como argumento para la construcción de muchos modelos tales como LSA, LDA, etc.\n",
    "Por ejemplo, podemos inspeccionar cuál es el primer y el quinto término cargado en el diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1° Término:  aborto\n",
      "5° Término:  acompañó\n"
     ]
    }
   ],
   "source": [
    "print u'1° Término: ', dictionary[0]\n",
    "print u'5° Término: ', dictionary[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En forma inversa podemos ver cuál es el índice asociado a un determinado término mediante el atributo *token2id* (que es a su vez un [*diccionario* de python](http://entrenamiento-python-basico.readthedocs.io/es/latest/leccion3/)), en caso que esté incorporado en el diccionario. Por ejemplo, buscamos los índices asociados a los términos *aborto* y *macri*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aborto:  0\n",
      "macri:  1031\n"
     ]
    }
   ],
   "source": [
    "print 'aborto: ', dictionary.token2id['aborto']\n",
    "print 'macri: ', dictionary.token2id['macri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra gran utilidad del diccionario es su capacidad de convertir los textos en una bolsa de palabras o *bow* (*bag of words*). Una bolsa de palabras consiste en describir los documentos específicando los términos que aparecen y la frecuencia con la que lo hacen. Tomemos por ejemplo la siguiente frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = u'Despenalización del aborto: argumentos a favor y en contra de la despenalización.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos hacer siempre el preprocesamiento de minimizar la frase y dividirlo en palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'despenalizaci\\xf3n', u'del', u'aborto', u'argumentos', u'a', u'favor', u'y', u'en', u'contra', u'de', u'la', u'despenalizaci\\xf3n']\n"
     ]
    }
   ],
   "source": [
    "example = list(tokenize(example.lower()))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es invocar el método *doc2bow* del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (75, 1), (575, 1), (1267, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo descrito como una bag of words\n",
    "\n",
    "bow = dictionary.doc2bow(example)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta descripción es una lista de tuplas del estilo (índice del término,frecuencia de aparación). La lista de tuplas es una forma más eficiente de describir el documento, como alternativa a escribir en forma completa el vector documento en el espacio de términos. Dado que un documento solo contiene unos pocos términos en relación con la cantidad de términos presentes en el diccionario, el vector (y la matriz si pensamos en todo el corpus) es muy *sparse*, por lo tanto solo se indican los términos que tienen frecuencias no nulas.\n",
    "\n",
    "Vemos cómo queda entonces la descripción del ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra: aborto. Frecuencia: 1\n",
      "Palabra: favor. Frecuencia: 1\n",
      "Palabra: argumentos. Frecuencia: 1\n",
      "Palabra: despenalización. Frecuencia: 2\n"
     ]
    }
   ],
   "source": [
    "for b in bow:\n",
    "    print(u'Palabra: {}. Frecuencia: {}'.format(dictionary[b[0]], b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que las palabras comunes fueron descartadas al no incluirlas originalmente en el diccionario.\n",
    "\n",
    "### Salvado del corpus como *bag of words*\n",
    "\n",
    "Finalmente utilizemos el diccionario para dar una descripción del tipo *bag of words* de todo el corpus. Guardamos este objeto con la librería *cPickle* que nos permite guardar y cargar cualquier objeto creado en python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus como bag of words\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Salvamos el corpus\n",
    "import cPickle as pk\n",
    "pk.dump(corpus, file('Tutorial_corpus.pk','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos cargar el corpus más adelante mediante el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pk.load(file('Tutorial_corpus.pk','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse document frequency (Tf-idf)\n",
    "\n",
    "Cuando uno describe un documento como una bolsa de palabras podemos tener el problema que las palabras que aparecen con mucha frecuencia tienen mucho peso en el documento y aún así ser no representativas. Si bien ya realizamos un filtrado de palabras comunes como preposiciones o artículos este problema está latente. \n",
    "Para solucionarlo podemos aplicar el algoritmo **TF-idf** al corpus descrito como *bag of words* que le da más peso a aquellas palabras que son muy específicas e importantes para la definición de un tópico por ejemplo, pero aún así no son palabras citadas frecuentemente. Por otro lado permite quitarle importancia a aquellas palabras que son comunes en diversos documentos, y por lo tanto lograr mayor especificidad.\n",
    "\n",
    "**Gensim** calcula el peso del término $i$ en el documento $j$ mediante la siguiente fórmula, donde $f_{ij}$ es la frecuencia con la que aparece el $i$ en $j$ y $df_{i}$ (*document frequency*) es la cantidad de documentos donde aparece el término $i$:\n",
    "\n",
    "$$ w_{ij} = f_{ij} \\cdot log_2 \\frac{D}{df_{i}} $$\n",
    "\n",
    "Notar que si un término aparece en todos los documentos del corpus su peso solo estará dado por la frecuencia con la que aparece en cada documento. En cambio si un término es muy específico para un subconjunto de documentos tendrá en ellos un peso mayor.\n",
    "\n",
    "Por otro lado, una vez recalculados los pesos de cada término, se suele normalizar los vectores a norma euclidea 1. Por defecto **Gensim** lo hace, ya que puede ayudar a mejorar la comparación entre textos de distinto largo. Esta opción también puede tenerse en cuenta cuando se construye el corpus como *bag of words*. En ambos casos depende de en qué algoritmo vamos a aplicar nuestra descripción del corpus y ahí tomar la desición de utilizar la normalización o no.\n",
    "\n",
    "Apliquemos **Tf-idf** en **Gensim** sobre nuestro corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Construimos el modelo tfidf con todo el corpus\n",
    "tfidf = TfidfModel(corpus, normalize=True)\n",
    "\n",
    "# Transformamos el corpus pesando cada componente con la fórmula tfidf\n",
    "corpus_tfidf =  tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que en las líneas anteriores podríamos haber construido el modelo **tfidf** con todo el corpus y luego transformar un porción más pequeña, si solo nos interesa analizar una parte de él. Cuanto más textos tenga el corpus con el que entrenamos el modelo, se destacarán más los términos específicos.\n",
    "\n",
    "Podemos ver como transforma el modelo **tfidf** en la frase que dimos de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.24405263283153336), (75, 0.32262003197123384), (575, 0.590180434039706), (1267, 0.6985998014998152)]\n"
     ]
    }
   ],
   "source": [
    "example = u'Despenalización del aborto: argumentos a favor y en contra de la despenalización.'\n",
    "example = list(tokenize(example.lower()))\n",
    "bow = dictionary.doc2bow(example)\n",
    "print(tfidf[bow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valorización de cada término depende mucho del corpus con el que entrenamos el modelo **tfidf**.\n",
    "\n",
    "Guardamos finalmente el corpus transformado con **tfidf** para utilizarlo posteriormente. Muchos objetos de **Gensim** tienen el método *save* incorporado, pero podemos utilizar nuevamente el módulo *cPickle*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(corpus_tfidf, file('Tutorial_corpus_tfidf.pk','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargarlo utilizamos el método *load*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = pk.load(file('Tutorial_corpus_tfidf.pk','r'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
