{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "Utilizemos el diccionario creado y el corpus de prueba para ver cómo implementar un modelo, en este caso **Non-Negative matrix factorization**. Vamos a utlizar tanto la implementación en **Gensim** (que es más fácil para obtener información de tipo semántica) como la de **Scikit-learn** que nos permite a acceder a más información numérica.\n",
    "\n",
    "Comenzamos cargando el diccionario y el corpus previamente construidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from gensim.corpora import Dictionary\n",
    "import cPickle as pk\n",
    "\n",
    "# Cargado del diccionario construido\n",
    "dictionary = Dictionary.load('tutorial.dict')\n",
    "\n",
    "# Cargado del corpus como bag or words\n",
    "corpus = pk.load(file('Tutorial_corpus.pk','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos utlizando la implementación en **Scikit-learn**, para ello lo primero que tenemos que hacer es transformar el corpus como una matriz *sparse* que **Scikit-learn** entiende. **Gensim** nos permite hacer esto fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "\n",
    "# Corpus para sklearn\n",
    "corpus2sklearn = corpus2csc(corpus).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transposición (*.T*) nos permite que la matriz quede armada de forma tal que cada documento sea un vector fila en el espacio de términos. Esto se ve inspeccionando las dimensiones de la matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (400, 18646)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones: {}'.format(corpus2sklearn.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la matriz tiene 400 filas que es el número de documentos cargados y 18646 términos que los describen.\n",
    "\n",
    "### NMF\n",
    "\n",
    "**Non Negative Matrix Factorization** es un tipo de descomposición matricial que en la mayoría de los casos no puede hacer en forma exacta. La idea es poder descomponer una matriz con elemenos no negativos como un producto de otras dos matrices compuestas también por elementos no negativos: \n",
    "\n",
    "$$ A^{(m \\times n)} = H^{(m \\times k)} \\cdot W^{(k \\times n)} $$\n",
    "\n",
    "donde $H$ y $W$ tienen todos sus elementos no negativos y $k$ es un parámetro que indica una dimensión latente y que debe ser elegido antes de realizar la descomposición.\n",
    "**NMF** es particularmente útil para la descomposición de un corpus de texto en un cierto número de tópicos, especificados por el parámetro $k$. La no negatividad de todas las matrices involucradas en el cálculo permite que la interpretación sea directa: si $A$ es una matriz de documentos por términos, $H$ contiene a los documentos descritos en la base de tópicos y $W$ a los tópicos descritos en el espacio de términos.\n",
    "\n",
    "Vamos a utilizar **Scikit-learn** para la aplicación de este modelo. \n",
    "El esquema de trabajo en **Scikit-learn** es generalmente el siguiente:\n",
    "- Importar la clase del modelo que queremos utilizar.\n",
    "- Creamos un objeto a partir de la clase con los parámetros que querramos.\n",
    "- Llamamos al método *fit_transform* del objeto creado pasandole como argumentos los datos, en nuestro caso la matriz asociada al corpus.\n",
    "\n",
    "Por lo tanto comenzamos importando la clase de NMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que nuestro corpus de prueba fue preparado especialmente compuesto por 4 tópicos, veamos si **NMF** es capaz de realizar el etiquetado en forma correcta. Partimos entonces definiendo un objeto *nmf* con 4 tópicos a partir de la clase importada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto nmf a partir de la clase NMF\n",
    "nmf = NMF(n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos la matriz del corpus a la nueva base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_transformed = nmf.fit_transform(corpus2sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la dimensiones de la matriz corpus transformada es ahora el número de documentos por el número de las nuevas dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (400, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones: {}'.format(corpus_transformed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dimensiones de la matriz transformada nos indica que la matriz de corpus transformado es efectivamente la matriz $H$ de nuestro algoritmo. Por ejemplo si inspeccionamos el primer elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32101068 0.00219835 0.         0.00639528]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el primer documento esta descrito por pesos no negativos en el espacio de los 4 tópicos. Correctamente normalizado podemos entender este vector como una distribución de probabilidad en el espacio de términos. La normalización podemos llevarla a cabo mediante **Scikit-learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Creamos un normalizador con norma 1, \n",
    "# esto es la suma de las componentes de un vector da 1,\n",
    "# no confundir con la norma euclídea.\n",
    "\n",
    "norm = Normalizer('l1')\n",
    "corpus_transformed = norm.fit_transform(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo a inspeccionar el primer elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97392743 0.00666968 0.         0.01940289]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera vemos que el primer elemento está asociado al primer tópico hallado por el algoritmo.\n",
    "En caso que querramos etiquetar cada documento con una única etiqueta basta hallar el elemento más grande de vector en el espacio de tópicos. Lo hacemos fácilmente con la función *argmax* de **Numpy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_predicted = [np.argmax(d) for d in corpus_transformed]\n",
    "\n",
    "print('Etiquetas: {}'.format(labels_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comparación de etiquetas\n",
    "\n",
    "Del corpus de prueba conocemos el etiquetado esperado: son 400 documentos con 4 tópicos esperados, ordenados de 100 en 100. Creamos entonces una lista con las etiquetas verdaderas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "labels_true = [0] * 100 + [1] * 100 + [2] * 100 + [3] * 100\n",
    "print(labels_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que en python el signo *+* utilizado con listas significa concatenación. Del mismo modo, multiplicar por 100 una lista implica en concatenar 100 veces es lista consigo misma.\n",
    "Para ver la concordancia de las etiquetas predichas y las etiquetas esperadas vemos información mutua entre ellas, que vale 0 cuando las etiquetas están perfectamente descorrelacionadas, y 1 cuando la coincidencia es perfecta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua: 0.898364619583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "nmi = normalized_mutual_info_score(labels_predicted, labels_true)\n",
    "print('Información mutua: {}'.format(nmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información mutua es muy cercana a 1 por lo vemos que **NMF** detecta muy bien los tópicos presentes corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretación de tópicos\n",
    "\n",
    "La información del significado de los tópicos quedó guardada en la matriz $W$ del algoritmo. Esta puede hallarse en el atributo *components_* del objeto *nmf*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (4, 18646)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones: {}'.format(nmf.components_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente *components_* es la matriz de los 4 tópicos descritas en el espacio de los términos originales. Las componentes más grandes de cada vector fila indicarán cuáles son los términos más relevantes a la hora de definir un tópico. Para ello debemos hacer dos cosas:\n",
    "\n",
    "- ordenar los índices de las componentes de los vectores tópicos según el peso de cada componente.\n",
    "- identificar qué término está asociado a cada índice\n",
    "\n",
    "Vamos paso a paso trabajando por ahora con el primer tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajo solo en el primer tópico\n",
    "c = nmf.components_[0]\n",
    "\n",
    "# El tamaño de c es el índice más alto (menos 1 en realidad ya que se cuenta desde 0)\n",
    "m = len(c)\n",
    "# Creo una lista que contengan todos los índices\n",
    "l = range(m)\n",
    "# Ordeno una lista que contenga los índices según el peso de cada componente ordenados de mayor a menor (reverse = True)\n",
    "ordered_index_list = sorted(l, reverse = True, key = lambda x: c[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El argumento *key* es quizás el más difícil de entender pero el más útil: es el criterio con el cual ordena. Se lee como: *x* es un elemento de la lista que se quiere ordenar. Estamos ordenando índices con el criterio de que los ubique según el valor la componente evaluado en dicho índice (*c[x]*). El hecho que los ordene de mayor a menor según este criterio está espedificado en el argumento *reverse = True*.\n",
    "Vemos cuáles son los primeros 10 índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 445, 117, 342, 322, 474, 468, 411, 51, 367]\n"
     ]
    }
   ],
   "source": [
    "print(ordered_index_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A qué términos están asociados estos índices lo vemos a traves del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aborto, vida, mujer, mujeres, ley, embarazo, debate, salud, derecho, persona\n"
     ]
    }
   ],
   "source": [
    "topic_terms = [dictionary[i] for i in ordered_index_list[:10]]\n",
    "\n",
    "# Unimos la lista en un string\n",
    "print(u', '.join(topic_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterando con el mismo procedimiento, obtengamos la descripción de todos los tópicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico 0: aborto, vida, mujer, mujeres, ley, embarazo, debate, salud, derecho, persona\n",
      "Tópico 1: trump, presidente, estados, unidos, casa, blanca, acuerdo, donald, países, washington\n",
      "Tópico 2: dólar, inflación, mercado, banco, cambio, central, suba, bcra, tasa, us\n",
      "Tópico 3: boca, equipo, partido, copa, guillermo, river, libertadores, pavón, barros, schelotto\n"
     ]
    }
   ],
   "source": [
    "nt = 0 # Indice auxiliar\n",
    "for c in nmf.components_:\n",
    "    \n",
    "    m = len(c)\n",
    "    l = range(m)\n",
    "    ordered_index_list = sorted(l, reverse = True, key = lambda x: c[x])\n",
    "    topic_terms = [dictionary[i] for i in ordered_index_list[:10]]\n",
    "    print(u'Tópico {}: '.format(nt) + u', '.join(topic_terms))\n",
    "\n",
    "    nt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que son efectivamente los tópicos que esperabamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
