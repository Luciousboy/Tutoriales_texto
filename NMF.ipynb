{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "Vamos a implementar un algoritmo de detección de tópicos denominado **Non-Negative matrix factorization**, basado en la descomposición de la matriz de *documentos x términos* como la multiplicación de dos matrices con elementos no negativos, que nos darán información sobre los tópicos presentes en cada documento y los términos que definen cada tópico respectivamente. Utilizaremos principalmente la implementación en la librería [**Scikit-learn**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html).\n",
    "\n",
    "Comenzamos cargando el diccionario y el corpus previamente construidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from gensim.corpora import Dictionary\n",
    "import cPickle as pk\n",
    "\n",
    "# Cargado del diccionario construido\n",
    "dictionary = Dictionary.load('tutorial.dict')\n",
    "\n",
    "# Cargado del corpus como bag or words\n",
    "corpus = pk.load(file('Tutorial_corpus.pk','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar la implementación en Scikit-learn necesitamos transformar el corpus en una matriz sparse que Scikit-learn entiende. Gensim nos permite hacer esto fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "\n",
    "# Corpus para sklearn\n",
    "corpus2sklearn = corpus2csc(corpus).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transposición (*.T*) nos permite que la matriz quede armada de forma tal que cada documento sea un vector fila en el espacio de términos. Esto se ve inspeccionando las dimensiones de la matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (400, 18646)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones: {}'.format(corpus2sklearn.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la matriz tiene 400 filas que es el número de documentos cargados y 18646 términos que los describen.\n",
    "\n",
    "### NMF\n",
    "\n",
    "[**Non Negative Matrix Factorization**](https://es.wikipedia.org/wiki/Factorizaci%C3%B3n_no_negativa_de_matrices) es un tipo de descomposición matricial que en la mayoría de los casos no puede hacerse de forma exacta. La idea es descomponer una matriz con elementos no negativos como un producto de otras dos matrices compuestas también por elementos no negativos: \n",
    "\n",
    "$$ A^{(m \\times n)} = H^{(m \\times k)} \\cdot W^{(k \\times n)} $$\n",
    "\n",
    "donde $H$ y $W$ tienen todos sus elementos no negativos y $k$ es un parámetro que indica una dimensión latente y que debe ser elegido antes de realizar la descomposición.\n",
    "**NMF** es particularmente útil para la descomposición de un corpus de texto en un cierto número de tópicos, especificados por el parámetro $k$. La no negatividad de todas las matrices involucradas en el cálculo permite que la interpretación sea directa: si $A$ es una matriz de documentos por términos, $H$ contiene a los documentos descritos en la base de tópicos y $W$ a los tópicos descritos en el espacio de términos.\n",
    "Por otro lado las componentes indican pesos: el peso de un tópico en un documento dado o bien el peso de un término en un determinado tópico.\n",
    "\n",
    "Vamos a utilizar **Scikit-learn** para la aplicación de este modelo. \n",
    "El esquema de trabajo en **Scikit-learn** es generalmente el siguiente:\n",
    "- Importar la clase del modelo que queremos utilizar.\n",
    "- Creamos un objeto a partir de la clase con los parámetros que querramos.\n",
    "- Llamamos al método *fit_transform* del objeto creado pasandole como argumentos los datos, en nuestro caso la matriz asociada al corpus.\n",
    "\n",
    "Por lo tanto comenzamos importando la clase de NMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que nuestro corpus de prueba fue preparado especialmente compuesto por 4 tópicos, veamos si **NMF** es capaz de realizar el etiquetado en forma correcta. Partimos entonces definiendo un objeto *nmf* con 4 tópicos a partir de la clase importada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto nmf a partir de la clase NMF\n",
    "nmf = NMF(n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos la matriz del corpus a la nueva base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación del corpus al espacio de 4 tópicos\n",
    "corpus_transformed = nmf.fit_transform(corpus2sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la dimensiones de la matriz corpus transformada es ahora el número de documentos por el número de las nuevas dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (400, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones: {}'.format(corpus_transformed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dimensiones de la matriz transformada nos indica que la matriz de corpus transformado es efectivamente la matriz $H$ de nuestro algoritmo. Por ejemplo si inspeccionamos el primer elemento, obtendremos el peso de cada tópico en el primer documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32101068 0.00219835 0.         0.00639528]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente el primer documento esta descrito por pesos no negativos en el espacio de los 4 tópicos. Correctamente normalizado podemos entender este vector como una distribución de probabilidad en el espacio de tópicos. La normalización podemos llevarla a cabo mediante **Scikit-learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Creamos un normalizador con norma 1, \n",
    "# esto es la suma de las componentes de un vector da 1,\n",
    "# no confundir con la norma euclídea.\n",
    "\n",
    "norm = Normalizer('l1')\n",
    "\n",
    "# Matriz documentos por tópicos con las filas normalizadas\n",
    "corpus_transformed = norm.fit_transform(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo a inspeccionar el primer elemento, vemos la distribución de tópicos sobre el primer documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97392742 0.00666968 0.         0.01940291]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera observamos que el primer elemento está asociado al primer tópico hallado por el algoritmo.\n",
    "En caso que querramos etiquetar cada documento con una única etiqueta basta hallar el elemento más grande del vector en el espacio de tópicos. Lo hacemos fácilmente con la función *argmax* de **Numpy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_predicted = [np.argmax(d) for d in corpus_transformed]\n",
    "\n",
    "print('Etiquetas: {}'.format(labels_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comparación de etiquetas\n",
    "\n",
    "El corpus de prueba fue especialmente preparado con 400 documentos pertenecientes a 4 tópicos, ordenados de 100 en 100. Para realizar la comparación con el etiquetado propuesto por **NMF**, creamos una lista con las etiquetas verdaderas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "labels_true = [0] * 100 + [1] * 100 + [2] * 100 + [3] * 100\n",
    "print(labels_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que en python el signo *+* utilizado con listas significa concatenación. Del mismo modo, multiplicar por 100 una lista implica en concatenar 100 veces la lista consigo misma.\n",
    "Para ver la concordancia de las etiquetas predichas y las etiquetas esperadas vemos la [información mutua](https://es.wikipedia.org/wiki/Informaci%C3%B3n_mutua) entre ellas, que vale 0 cuando las etiquetas están perfectamente descorrelacionadas y 1 cuando la coincidencia es perfecta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua: 0.898364619583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "nmi = normalized_mutual_info_score(labels_predicted, labels_true)\n",
    "print('Información mutua: {}'.format(nmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente observamos que la información mutua es muy cercana a 1 por lo que concluimos que **NMF** detecta muy bien los tópicos presentes corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretación de tópicos\n",
    "\n",
    "La información del significado de los tópicos quedó guardada en la matriz $W$ del algoritmo. Esta puede hallarse en el atributo *components_* del objeto *nmf*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (4, 18646)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones: {}'.format(nmf.components_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente *components_* es la matriz de los 4 tópicos descritas en el espacio de los términos originales. Las componentes más grandes de cada vector fila indicarán cuáles son los términos más relevantes a la hora de definir un tópico. Para ello debemos hacer dos cosas:\n",
    "\n",
    "- ordenar los índices de las componentes de los vectores tópicos según el peso de cada componente.\n",
    "- identificar qué término está asociado a cada índice\n",
    "\n",
    "Vamos paso a paso trabajando por ahora con el primer tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajo solo en el primer tópico\n",
    "c = nmf.components_[0]\n",
    "\n",
    "# El tamaño de c es el índice más alto (menos 1 en realidad ya que se cuenta desde 0)\n",
    "m = len(c)\n",
    "# Creo una lista que contengan todos los índices\n",
    "l = range(m)\n",
    "# Ordeno una lista que contenga los índices según el peso de cada componente ordenados de mayor a menor (reverse = True)\n",
    "ordered_index_list = sorted(l, reverse = True, key = lambda x: c[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El argumento *key* de la función *sorted* (ordenado) es quizás el más difícil de entender pero es simplemente el criterio con el cual ordena y se lee de la siguente manera: $x$ es un elemento de la lista que se quiere ordenar y estamos ordenando índices con el criterio de que los ubique según el valor de la componente evaluado en el mismo ($c[x]$). A mayor valor de $c[x]$, el índice $x$ estará mejor rankeado. El hecho que los ordene de mayor a menor según este criterio está especificado en el argumento *reverse = True*.\n",
    "Vemos cuáles son los primeros 10 índices del primer tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 445, 117, 342, 322, 474, 468, 411, 51, 367]\n"
     ]
    }
   ],
   "source": [
    "print(ordered_index_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A qué términos están asociados estos índices lo vemos a traves del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aborto, vida, mujer, mujeres, ley, embarazo, debate, salud, derecho, persona\n"
     ]
    }
   ],
   "source": [
    "# Lista con los términos asociados a cada índice\n",
    "topic_terms = [dictionary[i] for i in ordered_index_list[:10]]\n",
    "\n",
    "# Unimos la lista en un string\n",
    "print(u', '.join(topic_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterando con el mismo procedimiento, obtenemos la descripción de todos los tópicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico 0: aborto, vida, mujer, mujeres, ley, embarazo, debate, salud, derecho, persona\n",
      "Tópico 1: trump, presidente, estados, unidos, casa, blanca, acuerdo, donald, países, washington\n",
      "Tópico 2: dólar, inflación, mercado, banco, cambio, central, suba, bcra, tasa, us\n",
      "Tópico 3: boca, equipo, partido, copa, guillermo, river, libertadores, pavón, barros, schelotto\n"
     ]
    }
   ],
   "source": [
    "nt = 0 # Indice auxiliar\n",
    "for c in nmf.components_:\n",
    "    \n",
    "    m = len(c)\n",
    "    l = range(m)\n",
    "    ordered_index_list = sorted(l, reverse = True, key = lambda x: c[x])\n",
    "    topic_terms = [dictionary[i] for i in ordered_index_list[:10]]\n",
    "    print(u'Tópico {}: '.format(nt) + u', '.join(topic_terms))\n",
    "\n",
    "    nt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los términos de arriba pueden ser rápidamente asociados a los tópicos predefinidos a la hora de construir el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf\n",
    "\n",
    "Si bien obtuvimos muy buenos resultados con el corpus de prueba recordemos que el mismo fue preparado especialmente con 4 tópicos bastante disímiles entre si. Una ayuda que podemos darle al algoritmo de detección de tópicos es indicarle que términos son más importantes o especiales a priori. Esto ya lo habíamos especificado a partir de la construcción del corpus como una matriz **Tfidf**. Por lo tanto, aplicaremos **NMF** ahora sobre la descripción **Tfidf** de nuestro corpus. El esquema es análogo a lo hecho anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua: 0.873102088834\n",
      "Tópico 0: aborto, debate, diputados, proyecto, mujeres, salud, vida, discusión, legalización, mujer\n",
      "Tópico 1: dólar, centavos, mercado, bcra, banco, us, inflación, central, suba, cambio\n",
      "Tópico 2: boca, river, copa, equipo, pavón, partido, superliga, libertadores, guillermo, barros\n",
      "Tópico 3: trump, kim, cumbre, corea, estados, unidos, norte, blanca, presidente, aranceles\n"
     ]
    }
   ],
   "source": [
    "# Cargado del corpus como tfidf\n",
    "corpus_tfidf = pk.load(file('Tutorial_corpus_tfidf.pk','r'))\n",
    "\n",
    "# Corpus para sklearn\n",
    "corpus2sklearn = corpus2csc(corpus_tfidf).T\n",
    "\n",
    "# Objeto nmf a partir de la clase NMF\n",
    "nmf = NMF(n_components=4)\n",
    "\n",
    "# Transformamos el corpus como tfidf al espacio de tópicos\n",
    "corpus_transformed = nmf.fit_transform(corpus2sklearn)\n",
    "\n",
    "# Vemos las etiquetas predichas por el algoritmo\n",
    "labels_predicted = [np.argmax(d) for d in corpus_transformed]\n",
    "\n",
    "# Comparamos el etiquetado de nmf con las verdaderas mediante información mutua\n",
    "nmi = normalized_mutual_info_score(labels_predicted, labels_true)\n",
    "print('Información mutua: {}'.format(nmi))\n",
    "\n",
    "# Imprimimos los términos de cada tópico\n",
    "nt = 0 # Indice auxiliar\n",
    "for c in nmf.components_:\n",
    "    \n",
    "    m = len(c)\n",
    "    l = range(m)\n",
    "    ordered_index_list = sorted(l, reverse = True, key = lambda x: c[x])\n",
    "    topic_terms = [dictionary[i] for i in ordered_index_list[:10]]\n",
    "    print(u'Tópico {}: '.format(nt) + u', '.join(topic_terms))\n",
    "\n",
    "    nt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que la información mutua no varió demasiado (esto es de vuelta, debido a que el corpus fue preparado especialmente para que sea fácil detectar los tópicos), podemos observar que algunas palabras que aparecen en la definición de los tópicos son más específicas del contexto de cada uno. La aplicación de **Tfidf** generalmente ayuda a mejorar bastante la performance de algunos algoritmos, en particular de **NMF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- Proponer criterio para elegir la cantidad de tópicos en la descomposición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias:\n",
    "\n",
    "- [Lee, Daniel D., and H. Sebastian Seung. \"Learning the parts of objects by non-negative matrix factorization.\" Nature 401.6755 (1999): 788.](https://www.nature.com/articles/44565)\n",
    "- [Xu, Wei, Xin Liu, and Yihong Gong. \"Document clustering based on non-negative matrix factorization.\" Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM, 2003.](https://dl.acm.org/citation.cfm?id=860485)\n",
    "- [Lee, Daniel D., and H. Sebastian Seung. \"Algorithms for non-negative matrix factorization.\" Advances in neural information processing systems. 2001.](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
